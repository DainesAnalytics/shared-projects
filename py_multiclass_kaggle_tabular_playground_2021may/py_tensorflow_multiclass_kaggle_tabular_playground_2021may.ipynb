{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "py_tensorflow_multiclass_kaggle_tabular_playground_2021may.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "K_ha_1jD3l7-"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVs0Qopg3l7w"
      },
      "source": [
        "### Multi-Class Model for Kaggle Tabular Playground Series May 2021 Using Python and TensorFlow\n",
        "### David Lowe\n",
        "### July 8, 2021\n",
        "\n",
        "Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]\n",
        "\n",
        "SUMMARY: This project aims to construct a predictive model using various machine learning algorithms and document the end-to-end steps using a template. The Kaggle Tabular Playground May 2021 dataset is a multi-class modeling situation where we attempt to predict one of several (more than two) possible outcomes.\n",
        "\n",
        "INTRODUCTION: Kaggle wants to provide an approachable environment for relatively new people in their data science journey. Since January 2021, they have hosted playground-style competitions on Kaggle with fun but less complex, tabular datasets. The dataset used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n",
        "\n",
        "ANALYSIS: The performance of the cross-validated TensorFlow models achieved an average logarithmic loss benchmark of 1.1147 after running for ten epochs. When we applied the final model to Kaggle's test dataset, the model achieved a logarithmic loss score of 1.1155.\n",
        "\n",
        "CONCLUSION: In this iteration, the TensorFlow model appeared to be a suitable algorithm for modeling this dataset.\n",
        "\n",
        "Dataset Used: Kaggle Tabular Playground 2021 May Data Set\n",
        "\n",
        "Dataset ML Model: Multi-Class classification with categorical attributes\n",
        "\n",
        "Dataset Reference: https://www.kaggle.com/c/tabular-playground-series-may-2021/\n",
        "\n",
        "One potential source of performance benchmark: https://www.kaggle.com/c/tabular-playground-series-may-2021/leaderboard\n",
        "\n",
        "Any predictive modeling machine learning project generally can be broken down into about six major tasks:\n",
        "\n",
        "1. Prepare Environment\n",
        "2. Summarize and Visualize Data\n",
        "3. Pre-process Data\n",
        "4. Train and Evaluate Models\n",
        "5. Fine-tune and Improve Models\n",
        "6. Finalize Model and Present Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcvIFuk03l7z"
      },
      "source": [
        "## Task 1 - Prepare Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8WmuNd23l70"
      },
      "source": [
        "# Install the necessary packages for Colab\n",
        "# !pip install python-dotenv PyMySQL"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFWd7eph3l70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45abe0c1-7913-43ae-8208-34c70f0ea65d"
      },
      "source": [
        "# Retrieve the GPU information from Colab\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "    print('and then re-execute this cell.')\n",
        "else:\n",
        "    print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul  7 23:12:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tWJ-QK63l71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dafd8a9-dba4-4ec7-ec00-bea343d6b647"
      },
      "source": [
        "# Retrieve the memory configuration from Colab\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "    print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "    print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "    print('re-execute this cell.')\n",
        "else:\n",
        "    print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"\n",
            "menu, and then select High-RAM in the Runtime shape dropdown. Then, \n",
            "re-execute this cell.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMxv3rIM3l71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a776b9f-c0d1-4343-84e9-066417b0834d"
      },
      "source": [
        "# Retrieve the CPU information\n",
        "ncpu = !nproc\n",
        "print(\"The number of available CPUs is:\", ncpu[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of available CPUs is: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7LCAiYF3l72"
      },
      "source": [
        "### 1.a) Load libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcBsRRuj3l73"
      },
      "source": [
        "# Set the random seed number for reproducible results\n",
        "RNG_SEED = 888"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvqMowk53l73"
      },
      "source": [
        "import random\n",
        "random.seed(RNG_SEED)\n",
        "import numpy as np\n",
        "np.random.seed(RNG_SEED)\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "from datetime import datetime\n",
        "# import boto3\n",
        "# from dotenv import load_dotenv\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import compose\n",
        "from sklearn import impute\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(RNG_SEED)\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj9CEmZv3l74"
      },
      "source": [
        "### 1.b) Set up the controlling parameters and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37e7yBJt3l74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e3adb6-c291-423e-a81b-ee687e7c7fe9"
      },
      "source": [
        "# Begin the timer for the script processing\n",
        "start_time_script = datetime.now()\n",
        "\n",
        "# Set up the number of CPU cores available for multi-thread processing\n",
        "N_JOBS = 1\n",
        "\n",
        "# Set up the flag to stop sending progress emails (setting to True will send status emails!)\n",
        "NOTIFY_STATUS = False\n",
        "\n",
        "# Set the percentage sizes for splitting the dataset\n",
        "TEST_SET_RATIO = 0.2\n",
        "VAL_SET_RATIO = 0.2\n",
        "\n",
        "# Set the number of folds for cross validation\n",
        "N_FOLDS = 10\n",
        "N_ITERATIONS = 1\n",
        "\n",
        "# Set various default modeling parameters\n",
        "DEFAULT_LOSS = 'categorical_crossentropy'\n",
        "DEFAULT_METRICS = ['categorical_crossentropy']\n",
        "DEFAULT_OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "DEFAULT_INITIALIZER = tf.keras.initializers.RandomNormal(seed=RNG_SEED)\n",
        "DEFAULT_CLASSIFIER = 'softmax'\n",
        "MAX_EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# Define the labels to use for graphing the data\n",
        "train_metric = \"accuracy\"\n",
        "validation_metric = \"val_accuracy\"\n",
        "train_loss = \"loss\"\n",
        "validation_loss = \"val_loss\"\n",
        "\n",
        "# Check the number of GPUs accessible through TensorFlow\n",
        "print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Print out the TensorFlow version for confirmation\n",
        "print('TensorFlow version:', tf.__version__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available: 1\n",
            "TensorFlow version: 2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeabrzoC3l75"
      },
      "source": [
        "# Set up the email notification function\n",
        "def status_notify(msg_text):\n",
        "    access_key = os.environ.get('SNS_ACCESS_KEY')\n",
        "    secret_key = os.environ.get('SNS_SECRET_KEY')\n",
        "    aws_region = os.environ.get('SNS_AWS_REGION')\n",
        "    topic_arn = os.environ.get('SNS_TOPIC_ARN')\n",
        "    if (access_key is None) or (secret_key is None) or (aws_region is None):\n",
        "        sys.exit(\"Incomplete notification setup info. Script Processing Aborted!!!\")\n",
        "    sns = boto3.client('sns', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=aws_region)\n",
        "    response = sns.publish(TopicArn=topic_arn, Message=msg_text)\n",
        "    if response['ResponseMetadata']['HTTPStatusCode'] != 200 :\n",
        "        print('Status notification not OK with HTTP status code:', response['ResponseMetadata']['HTTPStatusCode'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6at5jHEyl6k"
      },
      "source": [
        "# Reset the random number generators\n",
        "def reset_random(x=RNG_SEED):\n",
        "    random.seed(x)\n",
        "    np.random.seed(x)\n",
        "    tf.random.set_seed(x)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LXEI2QY3l75"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 1 - Prepare Environment has begun! \" + datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iud46_Nn3l76"
      },
      "source": [
        "### 1.c) Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw8vysCp3l76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58242cf8-819c-4d76-b542-4befe115ad4c"
      },
      "source": [
        "dataset_path = 'https://dainesanalytics.com/datasets/kaggle-tabular-playground-2021may/train.csv'\n",
        "df_dataset_import = pd.read_csv(dataset_path, index_col=False)\n",
        "\n",
        "# Take a peek at the dataframe after import\n",
        "print(df_dataset_import.head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  feature_0  feature_1  ...  feature_48  feature_49   target\n",
            "0   0          0          0  ...           0           0  Class_2\n",
            "1   1          0          0  ...           0           0  Class_1\n",
            "2   2          0          0  ...           2           0  Class_1\n",
            "3   3          0          0  ...           1           0  Class_4\n",
            "4   4          0          0  ...           1           0  Class_2\n",
            "\n",
            "[5 rows x 52 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdseXAzd3l76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51586c78-60b6-49a5-be6b-f0a0f17e0368"
      },
      "source": [
        "df_dataset_import.info(verbose=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 52 columns):\n",
            " #   Column      Non-Null Count   Dtype \n",
            "---  ------      --------------   ----- \n",
            " 0   id          100000 non-null  int64 \n",
            " 1   feature_0   100000 non-null  int64 \n",
            " 2   feature_1   100000 non-null  int64 \n",
            " 3   feature_2   100000 non-null  int64 \n",
            " 4   feature_3   100000 non-null  int64 \n",
            " 5   feature_4   100000 non-null  int64 \n",
            " 6   feature_5   100000 non-null  int64 \n",
            " 7   feature_6   100000 non-null  int64 \n",
            " 8   feature_7   100000 non-null  int64 \n",
            " 9   feature_8   100000 non-null  int64 \n",
            " 10  feature_9   100000 non-null  int64 \n",
            " 11  feature_10  100000 non-null  int64 \n",
            " 12  feature_11  100000 non-null  int64 \n",
            " 13  feature_12  100000 non-null  int64 \n",
            " 14  feature_13  100000 non-null  int64 \n",
            " 15  feature_14  100000 non-null  int64 \n",
            " 16  feature_15  100000 non-null  int64 \n",
            " 17  feature_16  100000 non-null  int64 \n",
            " 18  feature_17  100000 non-null  int64 \n",
            " 19  feature_18  100000 non-null  int64 \n",
            " 20  feature_19  100000 non-null  int64 \n",
            " 21  feature_20  100000 non-null  int64 \n",
            " 22  feature_21  100000 non-null  int64 \n",
            " 23  feature_22  100000 non-null  int64 \n",
            " 24  feature_23  100000 non-null  int64 \n",
            " 25  feature_24  100000 non-null  int64 \n",
            " 26  feature_25  100000 non-null  int64 \n",
            " 27  feature_26  100000 non-null  int64 \n",
            " 28  feature_27  100000 non-null  int64 \n",
            " 29  feature_28  100000 non-null  int64 \n",
            " 30  feature_29  100000 non-null  int64 \n",
            " 31  feature_30  100000 non-null  int64 \n",
            " 32  feature_31  100000 non-null  int64 \n",
            " 33  feature_32  100000 non-null  int64 \n",
            " 34  feature_33  100000 non-null  int64 \n",
            " 35  feature_34  100000 non-null  int64 \n",
            " 36  feature_35  100000 non-null  int64 \n",
            " 37  feature_36  100000 non-null  int64 \n",
            " 38  feature_37  100000 non-null  int64 \n",
            " 39  feature_38  100000 non-null  int64 \n",
            " 40  feature_39  100000 non-null  int64 \n",
            " 41  feature_40  100000 non-null  int64 \n",
            " 42  feature_41  100000 non-null  int64 \n",
            " 43  feature_42  100000 non-null  int64 \n",
            " 44  feature_43  100000 non-null  int64 \n",
            " 45  feature_44  100000 non-null  int64 \n",
            " 46  feature_45  100000 non-null  int64 \n",
            " 47  feature_46  100000 non-null  int64 \n",
            " 48  feature_47  100000 non-null  int64 \n",
            " 49  feature_48  100000 non-null  int64 \n",
            " 50  feature_49  100000 non-null  int64 \n",
            " 51  target      100000 non-null  object\n",
            "dtypes: int64(51), object(1)\n",
            "memory usage: 39.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dut08ci_3l77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3f2298-83bf-4330-d139-03f3b5e57f6c"
      },
      "source": [
        "print(df_dataset_import.describe())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  id      feature_0  ...     feature_48    feature_49\n",
            "count  100000.000000  100000.000000  ...  100000.000000  100000.00000\n",
            "mean    49999.500000       0.257830  ...       0.970850       0.55712\n",
            "std     28867.657797       0.929033  ...       2.576615       1.68093\n",
            "min         0.000000       0.000000  ...       0.000000       0.00000\n",
            "25%     24999.750000       0.000000  ...       0.000000       0.00000\n",
            "50%     49999.500000       0.000000  ...       0.000000       0.00000\n",
            "75%     74999.250000       0.000000  ...       1.000000       0.00000\n",
            "max     99999.000000      10.000000  ...      44.000000      20.00000\n",
            "\n",
            "[8 rows x 51 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j5yMdxL3l77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87dca421-543c-41a7-b6aa-f2b0a4155655"
      },
      "source": [
        "print(df_dataset_import.isnull().sum())\n",
        "print('Total number of NaN in the dataframe: ', df_dataset_import.isnull().sum().sum())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id            0\n",
            "feature_0     0\n",
            "feature_1     0\n",
            "feature_2     0\n",
            "feature_3     0\n",
            "feature_4     0\n",
            "feature_5     0\n",
            "feature_6     0\n",
            "feature_7     0\n",
            "feature_8     0\n",
            "feature_9     0\n",
            "feature_10    0\n",
            "feature_11    0\n",
            "feature_12    0\n",
            "feature_13    0\n",
            "feature_14    0\n",
            "feature_15    0\n",
            "feature_16    0\n",
            "feature_17    0\n",
            "feature_18    0\n",
            "feature_19    0\n",
            "feature_20    0\n",
            "feature_21    0\n",
            "feature_22    0\n",
            "feature_23    0\n",
            "feature_24    0\n",
            "feature_25    0\n",
            "feature_26    0\n",
            "feature_27    0\n",
            "feature_28    0\n",
            "feature_29    0\n",
            "feature_30    0\n",
            "feature_31    0\n",
            "feature_32    0\n",
            "feature_33    0\n",
            "feature_34    0\n",
            "feature_35    0\n",
            "feature_36    0\n",
            "feature_37    0\n",
            "feature_38    0\n",
            "feature_39    0\n",
            "feature_40    0\n",
            "feature_41    0\n",
            "feature_42    0\n",
            "feature_43    0\n",
            "feature_44    0\n",
            "feature_45    0\n",
            "feature_46    0\n",
            "feature_47    0\n",
            "feature_48    0\n",
            "feature_49    0\n",
            "target        0\n",
            "dtype: int64\n",
            "Total number of NaN in the dataframe:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxxn8gnj3l78"
      },
      "source": [
        "### 1.d) Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osjQPBQY3l78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0265d689-c51b-421f-f79f-1ae6b1824ed2"
      },
      "source": [
        "# Dropping features\n",
        "df_dataset_import.drop(columns=['id'], inplace=True)\n",
        "\n",
        "# Convert columns from one data type to another\n",
        "train_feature_list = list(df_dataset_import.columns)\n",
        "for feature in train_feature_list:\n",
        "    df_dataset_import[feature] = df_dataset_import[feature].astype('category')\n",
        "\n",
        "# Take a peek at the dataframe after cleaning\n",
        "print(df_dataset_import.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  feature_0 feature_1 feature_2  ... feature_48 feature_49   target\n",
            "0         0         0         1  ...          0          0  Class_2\n",
            "1         0         0         0  ...          0          0  Class_1\n",
            "2         0         0         0  ...          2          0  Class_1\n",
            "3         0         0         0  ...          1          0  Class_4\n",
            "4         0         0         0  ...          1          0  Class_2\n",
            "\n",
            "[5 rows x 51 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t36Yn9gl3l78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d080a6ff-f7e0-4c94-9aeb-87660f0b80e5"
      },
      "source": [
        "df_dataset_import.info(verbose=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 51 columns):\n",
            " #   Column      Non-Null Count   Dtype   \n",
            "---  ------      --------------   -----   \n",
            " 0   feature_0   100000 non-null  category\n",
            " 1   feature_1   100000 non-null  category\n",
            " 2   feature_2   100000 non-null  category\n",
            " 3   feature_3   100000 non-null  category\n",
            " 4   feature_4   100000 non-null  category\n",
            " 5   feature_5   100000 non-null  category\n",
            " 6   feature_6   100000 non-null  category\n",
            " 7   feature_7   100000 non-null  category\n",
            " 8   feature_8   100000 non-null  category\n",
            " 9   feature_9   100000 non-null  category\n",
            " 10  feature_10  100000 non-null  category\n",
            " 11  feature_11  100000 non-null  category\n",
            " 12  feature_12  100000 non-null  category\n",
            " 13  feature_13  100000 non-null  category\n",
            " 14  feature_14  100000 non-null  category\n",
            " 15  feature_15  100000 non-null  category\n",
            " 16  feature_16  100000 non-null  category\n",
            " 17  feature_17  100000 non-null  category\n",
            " 18  feature_18  100000 non-null  category\n",
            " 19  feature_19  100000 non-null  category\n",
            " 20  feature_20  100000 non-null  category\n",
            " 21  feature_21  100000 non-null  category\n",
            " 22  feature_22  100000 non-null  category\n",
            " 23  feature_23  100000 non-null  category\n",
            " 24  feature_24  100000 non-null  category\n",
            " 25  feature_25  100000 non-null  category\n",
            " 26  feature_26  100000 non-null  category\n",
            " 27  feature_27  100000 non-null  category\n",
            " 28  feature_28  100000 non-null  category\n",
            " 29  feature_29  100000 non-null  category\n",
            " 30  feature_30  100000 non-null  category\n",
            " 31  feature_31  100000 non-null  category\n",
            " 32  feature_32  100000 non-null  category\n",
            " 33  feature_33  100000 non-null  category\n",
            " 34  feature_34  100000 non-null  category\n",
            " 35  feature_35  100000 non-null  category\n",
            " 36  feature_36  100000 non-null  category\n",
            " 37  feature_37  100000 non-null  category\n",
            " 38  feature_38  100000 non-null  category\n",
            " 39  feature_39  100000 non-null  category\n",
            " 40  feature_40  100000 non-null  category\n",
            " 41  feature_41  100000 non-null  category\n",
            " 42  feature_42  100000 non-null  category\n",
            " 43  feature_43  100000 non-null  category\n",
            " 44  feature_44  100000 non-null  category\n",
            " 45  feature_45  100000 non-null  category\n",
            " 46  feature_46  100000 non-null  category\n",
            " 47  feature_47  100000 non-null  category\n",
            " 48  feature_48  100000 non-null  category\n",
            " 49  feature_49  100000 non-null  category\n",
            " 50  target      100000 non-null  category\n",
            "dtypes: category(51)\n",
            "memory usage: 4.9 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FFwZYXG3l79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8210c8b-98c8-4419-da4f-2f9907ca27ce"
      },
      "source": [
        "print(df_dataset_import.describe())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        feature_0  feature_1  feature_2  ...  feature_48  feature_49   target\n",
            "count      100000     100000     100000  ...      100000      100000   100000\n",
            "unique         11         31          7  ...          45          21        4\n",
            "top             0          0          0  ...           0           0  Class_2\n",
            "freq        88473      89014      93492  ...       67780       80514    57497\n",
            "\n",
            "[4 rows x 51 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QMi80kq3l79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9ea45e-59d7-4433-910c-a30970b4aad6"
      },
      "source": [
        "print(df_dataset_import.isnull().sum())\n",
        "print('Total number of NaN in the dataframe: ', df_dataset_import.isnull().sum().sum())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature_0     0\n",
            "feature_1     0\n",
            "feature_2     0\n",
            "feature_3     0\n",
            "feature_4     0\n",
            "feature_5     0\n",
            "feature_6     0\n",
            "feature_7     0\n",
            "feature_8     0\n",
            "feature_9     0\n",
            "feature_10    0\n",
            "feature_11    0\n",
            "feature_12    0\n",
            "feature_13    0\n",
            "feature_14    0\n",
            "feature_15    0\n",
            "feature_16    0\n",
            "feature_17    0\n",
            "feature_18    0\n",
            "feature_19    0\n",
            "feature_20    0\n",
            "feature_21    0\n",
            "feature_22    0\n",
            "feature_23    0\n",
            "feature_24    0\n",
            "feature_25    0\n",
            "feature_26    0\n",
            "feature_27    0\n",
            "feature_28    0\n",
            "feature_29    0\n",
            "feature_30    0\n",
            "feature_31    0\n",
            "feature_32    0\n",
            "feature_33    0\n",
            "feature_34    0\n",
            "feature_35    0\n",
            "feature_36    0\n",
            "feature_37    0\n",
            "feature_38    0\n",
            "feature_39    0\n",
            "feature_40    0\n",
            "feature_41    0\n",
            "feature_42    0\n",
            "feature_43    0\n",
            "feature_44    0\n",
            "feature_45    0\n",
            "feature_46    0\n",
            "feature_47    0\n",
            "feature_48    0\n",
            "feature_49    0\n",
            "target        0\n",
            "dtype: int64\n",
            "Total number of NaN in the dataframe:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uuh6PSn53l79"
      },
      "source": [
        "### 1.e) Splitting Data into Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac66Lgmn3l79"
      },
      "source": [
        "# Use variable total_column_count to hold the number of columns in the dataframe\n",
        "total_column_count = len(df_dataset_import.columns)\n",
        "\n",
        "# Set up variable total_feature_count for the total number of attribute columns\n",
        "total_feature_count = total_column_count-1\n",
        "\n",
        "# target_column_position variable indicates the column location of the target/class variable\n",
        "# If the first column, set target_column_position to 1. If the last column, set target_column_position to total_column_count\n",
        "# If (target_column_position <> 1) and (target_column_position <> total_column_count), be aware when slicing up the dataframes for visualization\n",
        "target_column_position = total_column_count"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okf69cJ-3l7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca7b1a89-1abb-43dc-f0d8-2644b3399ebf"
      },
      "source": [
        "# We create attribute-only and target-only datasets (df_features_train and df_target_train)\n",
        "# for various visualization and cleaning/transformation operations\n",
        "\n",
        "if target_column_position == total_column_count:\n",
        "    df_features_train = df_dataset_import.iloc[:,0:total_feature_count]\n",
        "    df_target_train = df_dataset_import.iloc[:,total_feature_count]\n",
        "else:\n",
        "    df_features_train = df_dataset_import.iloc[:,1:total_column_count]\n",
        "    df_target_train = df_dataset_import.iloc[:,0]\n",
        "\n",
        "print(\"df_dataset_import.shape: {} df_features_train.shape: {} df_target_train.shape: {}\".format(df_dataset_import.shape, df_features_train.shape, df_target_train.shape))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_dataset_import.shape: (100000, 51) df_features_train.shape: (100000, 50) df_target_train.shape: (100000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ha_1jD3l7-"
      },
      "source": [
        "### 1.f) Set up the parameters for data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZU3DZ3D3l7-"
      },
      "source": [
        "# Set up the number of row and columns for visualization display. display_rows * display_columns should be >= total_feature_count\n",
        "display_columns = 4\n",
        "if total_feature_count % display_columns == 0 :\n",
        "    display_rows = total_feature_count // display_columns\n",
        "else :\n",
        "    display_rows = (total_feature_count // display_columns) + 1\n",
        "\n",
        "# Set figure width to display the data visualization plots\n",
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = display_columns * 4\n",
        "fig_size[1] = display_rows * 4\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACP83VhH3l7-"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 1 - Prepare Environment completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZevOIQL3l7_"
      },
      "source": [
        "## Task 2 - Summarize and Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLt0NpWy3l7_"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 2 - Summarize and Visualize Data has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVURY0Ba3l7_"
      },
      "source": [
        "# # Histograms for each attribute\n",
        "# df_features_train.plot(kind='hist', subplots=True, layout=(display_rows, display_columns))\n",
        "# plt.show()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m92aROlh3l8A"
      },
      "source": [
        "# # Box and Whisker plot for each attribute\n",
        "# df_features_train.plot(kind='box', subplots=True, layout=(display_rows, display_columns))\n",
        "# plt.show()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r8q4sgU3l8A"
      },
      "source": [
        "# # Correlation matrix\n",
        "# fig = plt.figure(figsize=(20, 20))\n",
        "# correlations = df_features_train.corr(method='pearson')\n",
        "# sns.heatmap(correlations, annot=True, cmap=plt.cm.PuBu)\n",
        "# plt.show()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47zwOFBY3l8A"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 2 - Summarize and Visualize Data completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMRuoNay3l8B"
      },
      "source": [
        "## Task 3 - Pre-process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vQJ1B6d3l8B"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 3 - Pre-process Data has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLUO6zME3l8B"
      },
      "source": [
        "### 3.a) Splitting Data into Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vg3u6An3l8B"
      },
      "source": [
        "# Not applicable for this iteration of the project"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJKVMdU-3l8B"
      },
      "source": [
        "### 3.b) Feature Scaling and Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdJRYIRo3l8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1bf1739-60b2-4975-e188-11d658e20b6f"
      },
      "source": [
        "# Compose pipeline for the numerical and categorical features (Block #1 of 2)\n",
        "numeric_columns = df_features_train.select_dtypes(include=['int64','float64']).columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    # ('imputer', impute.SimpleImputer(strategy=\"median\")),\n",
        "    ('scaler', preprocessing.MinMaxScaler())\n",
        "])\n",
        "categorical_columns = df_features_train.select_dtypes(include=['object','bool','category']).columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    # ('imputer', impute.SimpleImputer(strategy='constant', fill_value='UKNOWN')),\n",
        "    ('onehot', preprocessing.OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "print(\"Number of numerical columns:\", len(numeric_columns))\n",
        "print(\"Number of categorical columns:\", len(categorical_columns))\n",
        "print(\"Total number of columns in the feature dataframe:\", df_features_train.shape[1])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of numerical columns: 0\n",
            "Number of categorical columns: 50\n",
            "Total number of columns in the feature dataframe: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyFhjc43l8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfbbb8c-8124-4365-b432-ba77e6bda3e8"
      },
      "source": [
        "# Compose pipeline for the numerical and categorical features (Block #2 of 2)\n",
        "preprocessor = compose.ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_columns),\n",
        "    ('cat', categorical_transformer, categorical_columns)\n",
        "])\n",
        "\n",
        "# Display the shapes of the training dataset for final inspection\n",
        "array_features_train = preprocessor.fit_transform(df_features_train)\n",
        "print(\"Transformed features from df_features_train.shape: {} to array_features_train.shape: {}\".format(df_features_train.shape, array_features_train.shape))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformed features from df_features_train.shape: (100000, 50) to array_features_train.shape: (100000, 1355)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj81IbIC3l8C"
      },
      "source": [
        "### 3.c) Training Data Balancing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke6jtd9V3l8C"
      },
      "source": [
        "# Not applicable for this iteration of the project"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR-zmIHD3l8C"
      },
      "source": [
        "### 3.d) Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTX0mnVe3l8D"
      },
      "source": [
        "# Not applicable for this iteration of the project"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGAvblp03l8D"
      },
      "source": [
        "### 3.e) Display the Final Datasets for Model-Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EourCr333l8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc13673f-9fdf-4e47-e747-36af2e5ea420"
      },
      "source": [
        "# Finalize the training and validation datasets for the modeling activities\n",
        "# array_features_train = df_features_train.to_numpy()\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "array_target_train = keras.utils.to_categorical(label_encoder.fit_transform(df_target_train))\n",
        "print(\"array_features_train.shape: {} array_target_train.shape: {}\".format(array_features_train.shape, array_target_train.shape))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array_features_train.shape: (100000, 1355) array_target_train.shape: (100000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_DPVC0t3l8D"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 3 - Pre-process Data completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4q-C4XU3l8D"
      },
      "source": [
        "## Task 4 - Train and Evaluate Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_zb8Pq13l8D"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 4 - Train and Evaluate Models has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbls2rTF3l8E"
      },
      "source": [
        "# Define the baseline model for benchmarking\n",
        "def create_nn_model(input_param=array_features_train.shape[1], output_param=NUM_CLASSES, layer1_nodes=512, layer2_nodes=384, layer3_nodes=256, layer4_nodes=128, layer5_nodes=64,\n",
        "                    layer1_dropout=0.25, layer2_dropout=0.25, layer3_dropout=0.25, layer4_dropout=0.25, layer5_dropout=0.25,\n",
        "                    init_param=DEFAULT_INITIALIZER, classifier_activation=DEFAULT_CLASSIFIER,\n",
        "                    loss_param=DEFAULT_LOSS, opt_param=DEFAULT_OPTIMIZER, metrics_param=DEFAULT_METRICS):\n",
        "    nn_model = keras.Sequential([\n",
        "        keras.layers.Dense(layer1_nodes, input_shape=(input_param,), activation='relu', kernel_initializer=init_param),\n",
        "        keras.layers.Dropout(layer1_dropout),\n",
        "        keras.layers.Dense(layer2_nodes, activation='relu', kernel_initializer=init_param),\n",
        "        keras.layers.Dropout(layer2_dropout),\n",
        "        keras.layers.Dense(layer3_nodes, activation='relu', kernel_initializer=init_param),\n",
        "        keras.layers.Dropout(layer3_dropout),\n",
        "        keras.layers.Dense(layer4_nodes, activation='relu', kernel_initializer=init_param),\n",
        "        keras.layers.Dropout(layer4_dropout),\n",
        "        keras.layers.Dense(layer5_nodes, activation='relu', kernel_initializer=init_param),\n",
        "        # keras.layers.Dropout(layer5_dropout),\n",
        "        keras.layers.Dense(output_param, activation=classifier_activation)\n",
        "    ])\n",
        "    nn_model.compile(loss=loss_param, optimizer=opt_param, metrics=metrics_param)\n",
        "    return nn_model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxAHVg7Q3l8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a60c38-9690-4e46-8c8a-b7affc61328a"
      },
      "source": [
        "# Initialize the default model and get a baseline result\n",
        "start_time_module = datetime.now()\n",
        "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.000001)\n",
        "results = list()\n",
        "iteration = 0\n",
        "cv = RepeatedKFold(n_splits=N_FOLDS, n_repeats=N_ITERATIONS, random_state=RNG_SEED)\n",
        "for train_ix, val_ix in cv.split(array_features_train):\n",
        "    feature_train, feature_validation = array_features_train[train_ix], array_features_train[val_ix]\n",
        "    target_train, target_validation = array_target_train[train_ix], array_target_train[val_ix]\n",
        "    reset_random()\n",
        "    baseline_model = create_nn_model()\n",
        "    baseline_model.fit(feature_train, target_train, epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
        "    model_metric = baseline_model.evaluate(feature_validation, target_validation, verbose=1)[1]\n",
        "    iteration = iteration + 1\n",
        "    print('Score from iteration %d >>> %.4f' % (iteration, model_metric))\n",
        "    results.append(model_metric)\n",
        "validation_score = np.mean(results)\n",
        "validation_variance = np.std(results)\n",
        "print('Average model scorer from all iterations: %.4f (%.4f)' % (validation_score, validation_variance))\n",
        "print('Total time for model fitting and cross validating:', (datetime.now() - start_time_module))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1433 - categorical_crossentropy: 1.1433\n",
            "Score from iteration 1 >>> 1.1433\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1021 - categorical_crossentropy: 1.1021\n",
            "Score from iteration 2 >>> 1.1021\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1167 - categorical_crossentropy: 1.1167\n",
            "Score from iteration 3 >>> 1.1167\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0988 - categorical_crossentropy: 1.0988\n",
            "Score from iteration 4 >>> 1.0988\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1104 - categorical_crossentropy: 1.1104\n",
            "Score from iteration 5 >>> 1.1104\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1080 - categorical_crossentropy: 1.1080\n",
            "Score from iteration 6 >>> 1.1080\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1052 - categorical_crossentropy: 1.1052\n",
            "Score from iteration 7 >>> 1.1052\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1154 - categorical_crossentropy: 1.1154\n",
            "Score from iteration 8 >>> 1.1154\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1374 - categorical_crossentropy: 1.1374\n",
            "Score from iteration 9 >>> 1.1374\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1094 - categorical_crossentropy: 1.1094\n",
            "Score from iteration 10 >>> 1.1094\n",
            "Average model scorer from all iterations: 1.1147 (0.0139)\n",
            "Total time for model fitting and cross validating: 0:09:34.263936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PxkOhNE3l8F"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 4 - Train and Evaluate Models completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIRMoNm63l8H"
      },
      "source": [
        "## Task 5 - Finalize Model and Present Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgw8i_ME3l8H"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 5 - Finalize Model and Present Analysis has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsQONLo33l8I"
      },
      "source": [
        "### 6.a) Train the Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIDASCL13l8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb3051c-6f94-4c26-ecd2-c89320863c85"
      },
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='loss', patience=2, verbose=1, factor=0.5, min_lr=0.000001)\n",
        "final_model = create_nn_model()\n",
        "final_model.fit(array_features_train, array_target_train, epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, callbacks=[learning_rate_reduction], verbose=1)\n",
        "final_model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.1119 - categorical_crossentropy: 1.1119\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.1008 - categorical_crossentropy: 1.1008\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.0972 - categorical_crossentropy: 1.0972\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.0935 - categorical_crossentropy: 1.0935\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.0897 - categorical_crossentropy: 1.0897\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.0841 - categorical_crossentropy: 1.0841\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.0796 - categorical_crossentropy: 1.0796\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.0717 - categorical_crossentropy: 1.0717\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.0655 - categorical_crossentropy: 1.0655\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 7s 2ms/step - loss: 1.0558 - categorical_crossentropy: 1.0558\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_60 (Dense)             (None, 512)               694272    \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 384)               196992    \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 384)               0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 256)               98560     \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 1,031,236\n",
            "Trainable params: 1,031,236\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzeEayxg3l8I"
      },
      "source": [
        "### 6.b) Load Test Dataset and Prepare the Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8S2vLNw3l8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46587184-1c3a-42b2-8eab-7571c804f63b"
      },
      "source": [
        "dataset_path = 'https://dainesanalytics.com/datasets/kaggle-tabular-playground-2021may/test.csv'\n",
        "df_features_test = pd.read_csv(dataset_path, index_col=False)\n",
        "\n",
        "# Take a peek at the dataframe after import\n",
        "print(df_features_test.head())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       id  feature_0  feature_1  ...  feature_47  feature_48  feature_49\n",
            "0  100000          0          0  ...           0           0           0\n",
            "1  100001          0          0  ...           0           2           1\n",
            "2  100002          0          0  ...           0           6           0\n",
            "3  100003          0          0  ...           9          14           3\n",
            "4  100004          0          0  ...           0           0           0\n",
            "\n",
            "[5 rows x 51 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVrEdBfT3l8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb1b923-e13c-4aa1-ef4e-15d984589901"
      },
      "source": [
        "df_kaggle_submission = pd.DataFrame()\n",
        "df_kaggle_submission['id'] = df_features_test['id']\n",
        "print(df_kaggle_submission.head())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       id\n",
            "0  100000\n",
            "1  100001\n",
            "2  100002\n",
            "3  100003\n",
            "4  100004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JPFel4u3l8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "302cff1b-65e3-4092-ac7f-d87dcacfa063"
      },
      "source": [
        "# Dropping features\n",
        "df_features_test.drop(columns=['id'], inplace=True)\n",
        "\n",
        "# Convert columns from one data type to another\n",
        "test_feature_list = list(df_features_test.columns)\n",
        "for feature in test_feature_list:\n",
        "    df_features_test[feature] = df_features_test[feature].astype('category')\n",
        "\n",
        "# Take a peek at the dataframe after cleaning\n",
        "print(df_features_test.head())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  feature_0 feature_1 feature_2  ... feature_47 feature_48 feature_49\n",
            "0         0         0         0  ...          0          0          0\n",
            "1         0         0         1  ...          0          2          1\n",
            "2         0         0         0  ...          0          6          0\n",
            "3         0         0         0  ...          9         14          3\n",
            "4         0         0         0  ...          0          0          0\n",
            "\n",
            "[5 rows x 50 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avu1nEAa3l8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e36d86-eac6-4533-f9df-b96ffac4137d"
      },
      "source": [
        "# Finalize the test dataset for the modeling testing\n",
        "array_features_test = preprocessor.transform(df_features_test)\n",
        "print(\"Transformed features from df_features_test.shape: {} to array_features_test.shape: {}\".format(df_features_test.shape, array_features_test.shape))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformed features from df_features_test.shape: (50000, 50) to array_features_test.shape: (50000, 1355)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5YNHp6S3l8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6762cafe-ddd7-4a6f-e5ca-55085883b2b7"
      },
      "source": [
        "# Make batched predictions\n",
        "test_predictions = final_model.predict(array_features_test)\n",
        "print(test_predictions)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8.3124824e-02 6.3706946e-01 1.4785931e-01 1.3194646e-01]\n",
            " [2.1480408e-02 9.7800261e-01 2.0995610e-04 3.0703377e-04]\n",
            " [8.5854463e-02 6.4000839e-01 1.4682658e-01 1.2731060e-01]\n",
            " ...\n",
            " [7.5568162e-02 5.7442617e-01 1.8599682e-01 1.6400887e-01]\n",
            " [7.7179134e-02 5.6402248e-01 1.8398197e-01 1.7481638e-01]\n",
            " [9.1407135e-02 5.1114535e-01 2.6813409e-01 1.2931342e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXmbWirt3l8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0becbbb0-be9a-48a4-916d-844ff0c1ef25"
      },
      "source": [
        "df_kaggle_submission[['Class_1','Class_2','Class_3','Class_4']] = test_predictions\n",
        "print(df_kaggle_submission.head())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       id   Class_1   Class_2   Class_3   Class_4\n",
            "0  100000  0.083125  0.637069  0.147859  0.131946\n",
            "1  100001  0.021480  0.978003  0.000210  0.000307\n",
            "2  100002  0.085854  0.640008  0.146827  0.127311\n",
            "3  100003  0.096831  0.513219  0.291339  0.098611\n",
            "4  100004  0.099861  0.585160  0.232634  0.082344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-czkeOmO3l8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374a933a-6ac9-46a8-d122-6a506058f819"
      },
      "source": [
        "submission_file = df_kaggle_submission.to_csv(header=True, index=False)\n",
        "filename = 'submission_' + datetime.now().strftime('%Y%m%d-%H%M') + '.csv'\n",
        "with open(filename, 'w') as f:\n",
        "    f.write(submission_file)\n",
        "    print('Completed writing output file: ' + filename)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed writing output file: submission_20210707-2326.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE7Ey5753l8K"
      },
      "source": [
        "if NOTIFY_STATUS: status_notify(\"Task 5 - Finalize Model and Present Analysis completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxfno4RT3l8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb0c2b7-54bf-4825-b0f2-fbb997200ebc"
      },
      "source": [
        "print ('Total time for the script:',(datetime.now() - start_time_script))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time for the script: 0:13:54.176201\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}